{"cells":[{"cell_type":"markdown","metadata":{"id":"S82lBJko77rN"},"source":["# Prof. Pedram Jahangiry\n","\n","You need to make a copy to your own Google drive if you want to edit the original notebook! Start by opening this notebook on Colab ðŸ‘‡\n","\n","<a href=\"https://colab.research.google.com/github/PJalgotrader/Deep_Learning-USU/blob/main/Lectures%20and%20codes/DL%20Spring%202023/Module%206-%20Deep%20Sequence%20Modeling/python/Module%206-%20NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> \n","\n","\n","\n","![logo](https://upload.wikimedia.org/wikipedia/commons/4/44/Huntsman-Wordmark-with-USU-Blue.gif#center) \n","\n","\n","## ðŸ”— Links\n","\n","[![linkedin](https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pedram-jahangiry-cfa-5778015a)\n","\n","[![Youtube](https://img.shields.io/badge/youtube_channel-1DA1F2?style=for-the-badge&logo=youtube&logoColor=white&color=FF0000)](https://www.youtube.com/channel/UCNDElcuuyX-2pSatVBDpJJQ)\n","\n","[![Twitter URL](https://img.shields.io/twitter/url/https/twitter.com/PedramJahangiry.svg?style=social&label=Follow%20%40PedramJahangiry)](https://twitter.com/PedramJahangiry)\n","\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qH7ii3gI26Oe"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this part of the code, we  define a custom layer for the transformer model called **MultiHeadSelfAttention**. This layer is responsible for implementing the multi-head self-attention mechanism, a key component in the transformer architecture."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WoDmcUSz26Og"},"outputs":[],"source":["# defining the transformer layer\n","class MultiHeadSelfAttention(layers.Layer): # Define a new class MultiHeadSelfAttention that inherits from the layers.Layer base class provided by Keras\n","    def __init__(self, embed_dim, num_heads=8): # The constructor for the class takes two parameters, embed_dim and num_heads. \n","        # embed_dim is the dimension of the input embeddings, and num_heads is the number of attention heads in the multi-head self-attention mechanism.\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        if embed_dim % num_heads != 0:\n","            raise ValueError(\n","                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n","            )\n","            \n","        \n","        self.projection_dim = embed_dim // num_heads\n","        \n","        # The followings are dense (fully connected) layers responsible for computing the query, key, and value matrices, respectively, from the input embeddings.\n","        self.query_dense = layers.Dense(embed_dim)\n","        self.key_dense = layers.Dense(embed_dim)\n","        self.value_dense = layers.Dense(embed_dim)\n","        \n","        # Another dense layer that combines the outputs from all attention heads.\n","        self.combine_heads = layers.Dense(embed_dim)\n","\n","    # This method calculates the attention scores, scales them, applies the softmax function to obtain the attention weights, \n","    # and then computes the output by multiplying the attention weights with the value matrix.\n","    def attention(self, query, key, value):\n","        score = tf.matmul(query, key, transpose_b=True)\n","        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) # tf.cast is a TensorFlow function used to change the data type of a tensor. \n","        # By using [-1], you are selecting the last element of the shape tensor. In this case, it corresponds to the dimension of the key vectors.\n","        \n","        scaled_score = score / tf.math.sqrt(dim_key)\n","        weights = tf.nn.softmax(scaled_score, axis=-1)\n","        output = tf.matmul(weights, value)\n","        return output, weights\n","\n","    def separate_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, inputs):\n","        batch_size = tf.shape(inputs)[0]\n","        query = self.query_dense(inputs)\n","        key = self.key_dense(inputs)\n","        value = self.value_dense(inputs)\n","        query = self.separate_heads(query, batch_size)\n","        key = self.separate_heads(key, batch_size)\n","        value = self.separate_heads(value, batch_size)\n","        attention, weights = self.attention(query, key, value)\n","        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n","        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n","        output = self.combine_heads(concat_attention)\n","        return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5nZFP9F26Og"},"outputs":[],"source":["# Define the Transformer block: # https://keras.io/examples/nlp/text_classification_with_transformer/ \n","class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super().__init__()\n","        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TVGyT9Gn26Oh"},"outputs":[],"source":["# Positional embedding\n","class TokenAndPositionEmbedding(layers.Layer): # https://keras.io/api/keras_nlp/modeling_layers/token_and_position_embedding/\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions\n","\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l2P9pUIA4glw"},"outputs":[],"source":["# defining the model\n","def create_transformer_model(maxlen, vocab_size, embed_dim, num_heads, ff_dim, num_blocks, num_classes, dropout_rate=0.1):\n","    inputs = layers.Input(shape=(maxlen,))\n","    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","    x = embedding_layer(inputs)\n","    for _ in range(num_blocks):\n","        x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n","    x = layers.GlobalAveragePooling1D()(x)\n","    x = layers.Dropout(dropout_rate)(x)\n","    x = layers.Dense(30, activation=\"relu\")(x)\n","    x = layers.Dropout(dropout_rate)(x)\n","    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n","\n","    return keras.Model(inputs=inputs, outputs=outputs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4550,"status":"ok","timestamp":1681780288265,"user":{"displayName":"Pedram Jahangiry","userId":"17912812838437468468"},"user_tz":360},"id":"XlGo0-SV26Oi","outputId":"13fd0a47-b3ad-4970-e040-928985d7803d"},"outputs":[{"name":"stdout","output_type":"stream","text":["25000 Training sequences\n","25000 Validation sequences\n"]}],"source":["# download and prep the data\n","vocab_size = 20000  # Only consider the top 20k words\n","maxlen = 600  # Only consider the first 600 words of each movie review\n","(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n","print(len(x_train), \"Training sequences\")\n","print(len(x_val), \"Validation sequences\")\n","x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1681780305904,"user":{"displayName":"Pedram Jahangiry","userId":"17912812838437468468"},"user_tz":360},"id":"feblWOM526Oj","outputId":"15a396d6-e0c1-4c9f-d0d4-ded731ca8917"},"outputs":[{"data":{"text/plain":["(25000, 600)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["x_train.shape"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":629127,"status":"ok","timestamp":1681781311619,"user":{"displayName":"Pedram Jahangiry","userId":"17912812838437468468"},"user_tz":360},"id":"pMgAadh826Oj","outputId":"99378372-29d9-4388-d7d2-ebbe1a2a2611"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","782/782 [==============================] - 132s 158ms/step - loss: 0.3884 - accuracy: 0.8148 - val_loss: 0.2956 - val_accuracy: 0.8740\n","Epoch 2/10\n","782/782 [==============================] - 72s 92ms/step - loss: 0.1787 - accuracy: 0.9330 - val_loss: 0.3039 - val_accuracy: 0.8752\n","Epoch 3/10\n","782/782 [==============================] - 64s 82ms/step - loss: 0.0818 - accuracy: 0.9719 - val_loss: 0.4965 - val_accuracy: 0.8583\n","Epoch 4/10\n","782/782 [==============================] - 49s 62ms/step - loss: 0.0441 - accuracy: 0.9851 - val_loss: 0.5265 - val_accuracy: 0.8606\n","Epoch 5/10\n","782/782 [==============================] - 45s 58ms/step - loss: 0.0262 - accuracy: 0.9912 - val_loss: 0.5986 - val_accuracy: 0.8601\n","Epoch 6/10\n","782/782 [==============================] - 53s 67ms/step - loss: 0.0230 - accuracy: 0.9920 - val_loss: 0.6379 - val_accuracy: 0.8499\n","Epoch 7/10\n","782/782 [==============================] - 53s 68ms/step - loss: 0.0190 - accuracy: 0.9936 - val_loss: 0.7063 - val_accuracy: 0.8594\n","Epoch 8/10\n","782/782 [==============================] - 42s 54ms/step - loss: 0.0145 - accuracy: 0.9953 - val_loss: 0.7128 - val_accuracy: 0.8521\n","Epoch 9/10\n","782/782 [==============================] - 52s 66ms/step - loss: 0.0147 - accuracy: 0.9953 - val_loss: 0.8040 - val_accuracy: 0.8532\n","Epoch 10/10\n","782/782 [==============================] - 50s 64ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 1.0391 - val_accuracy: 0.8442\n"]}],"source":["# train the model\n","embed_dim = 32\n","num_heads = 2\n","ff_dim = 32\n","num_blocks = 2\n","num_classes = 2\n","dropout_rate = 0.1\n","\n","model = create_transformer_model(maxlen, vocab_size, embed_dim, num_heads, ff_dim, num_blocks, num_classes, dropout_rate)\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(1e-3),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","    metrics=[\"accuracy\"],\n",")\n","\n","history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Links:\n","* Text classification with Transformers: https://keras.io/examples/nlp/text_classification_with_transformer/"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
